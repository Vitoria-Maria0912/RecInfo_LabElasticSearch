{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratório Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "O objetivo deste laboratório é explorar diferentes mecanismos de busca dentro e fora do Elastic Search!\n",
    "Para isto, iremos explorar uma base de verificações de notícia de uma agência de checagem, chamada [Lupa](https://lupa.uol.com.br/).\n",
    "O papel de uma agência de checagem é analisar uma notícia e verificar sua veracidade, gerar um veredito e retornar sua análise.\n",
    "\n",
    "Neste laboratório, iremos nos preocupar apenas com o texto da análise e em como recuperá-los através de diferentes estratégias de busca.\n",
    "\n",
    "## Tarefas\n",
    "### Tarefa 1\n",
    "- Cada uma das equipes receberá 2 queries fixas, chamadas de QF1 e QF2, e a tarefa do grupo será a criação de mais 2 queries, chamadas de QP1 e QP2. As queries devem seguir o formato de uma pergunta ou declaração textual e ela será usada na próxima tarefa para realizar buscas. As equipes podem se inspirar nas notícias fornecidas (presentes no csv da tarefa 2) para montá-las. Cada query deve ser composta por até 100 caracteres.\n",
    "- A equipe deve enviar suas QP1 e QP2 via o [formulário](https://forms.gle/b3AcG79oCRhw65CH7)\n",
    "\n",
    "### Tarefa 2\n",
    "- Após a criação das QP1 e QP2, cada equipe possuirá 4 queries (QF1; QF2; QP1 e QP2).\n",
    "- Cada grupo irá realizar 4 tipos de busca. Uma busca léxica com BM25 (pelo ElasticSearch), uma busca semântica (pelo ElasticSearch), uma busca híbrida (manualmente utilizando as duas buscas anteriores) e uma estratégia de busca de sua preferência (neste lab ela será chamada de busca criativa), diferente das anteriores.\n",
    "- As buscas léxica e semântica já estão implementadas, os grupos devem gerar a implementação das buscas híbridas e da busca criativa.\n",
    "- A busca criativa será a busca usada para a competição!\n",
    "- Cada grupo durante a implementação de suas buscas deve testar diferentes pré-processamentos nos dados para verificar como os resultados podem melhorar. Alguns pré-processamentos já foram implementados, mas os grupos não precisam se limitar aos pré-processamentos apresentados. Vocês podem buscar na internet implementações de outros pré-processamentos, criar padrões regex, etc. Observe que os dados precisarão ser reindexados no ElasticSearch sempre que os pré-processamentos forem modificados (inseridos, removidos ou mudados de ordem)!\n",
    "\n",
    "- Enquanto vocês testam seus pré-processamentos e algoritmos de busca, ao analisar os resultados obtidos para uma determinada query, anotem o nível de relevância de cada documento lido em relação à query. Observe que esse procedimento pode ser realizado continuamente durante o processo de implementação, pois a relevância de um documento para uma query independe de implementação de buscadores. Essa anotação deve ser realizada (online) em formato csv via [formulário](https://forms.gle/ipKHwaPQbkrYrFhZA), que será fornecido pela organização do Lab. No caso dos integrantes discordarem sobre a avaliação considerem a média dos valores.\n",
    "*ATENÇÃO! Para facilitar a avaliação de seu desempenho enquanto desenvolvem e a geração das anotações no formato correto, disponibilizamos esse [template](https://docs.google.com/spreadsheets/d/1fF0fuLgbIQu_5plOnI54iYwqywZuXKLYRUDeKaDCgKI/edit?usp=sharing) que inclui importação, avaliação, recuperação de gabarito e exportação. Crie uma cópia do template para usá-lo. Não é obrigatório usar o template.*\n",
    "    - A planilha que o time vai devolver tem o seguinte formato:\n",
    "        - doc_id, query_id, relevance\n",
    "        - 120, QP1, 2\n",
    "        - 487, QP1, 1\n",
    "        - ...\n",
    "    - Cada busca deve possuir pelo menos 10 resultados anotados.\n",
    "    - Não modifique o código que realiza o carregamento dos dados para que o doc_id seja consistente com os demais grupos!\n",
    "    - A rotulação deve possuir uma gradação em 3 níveis:\n",
    "        - 0: Não é Relevante\n",
    "        - 1: Pouco Relevante\n",
    "        - 2: Muito Relevante \n",
    "- A entrega desta tarefa será feita através de um formulário para entregar um .zip contendo todo o repositório e o código utilizado autocontido (com listagem das dependências utilizadas no requirements.txt se necessário) e suas buscas implementadas. Este código será reexecutado, então organize o código para que ele possa ser reexecutado em outra máquina.\n",
    "- A partir deste momento, nenhum grupo poderá alterar suas buscas novamente, então tenham ciência que esta será sua implementação final que será usada para a competição.\n",
    "\n",
    "### Tarefa 3\n",
    "- O grupo receberá (pelo slack) 3 queries adicionais (QA1; QA2 e QA3).\n",
    "- A tarefa do grupo será a execução do MESMO CÓDIGO submetido na tarefa anterior com estas 3 queries e a anotação dos dados (a planilha de anotações será disponibilizada pelo discord).\n",
    "- Nesta etapa, o grupo NÃO PODE FAZER ALTERAÇÕES nos pré-processamentos definidos ou nas implementações de suas buscas.\n",
    "- Quando esgotar o prazo de envio da tarefa a planilha será bloqueada para modificações.\n",
    "- IMPORTANTE: O seu código entregue na tarefa 2 será reexecutado com estas 3 queries (assim como você fez) para garantir que os resultados batem com os seus. Os grupos que mudarem suas buscas ou pré-processamentos na tarefa 3 terão sua nota penalizada e serão desclassificados da competição.\n",
    "\n",
    "## Competição\n",
    "A competição será realizada através da comparação dos resultados das buscas criativas em uma base de documentos (corpus) utilizada pela organização do Lab.\n",
    "Para que você atinja um bom desempenho na competição, é importante se atentar ao seu processo de criação de queries e na qualidade da rotulação dos documentos, pois elas serão seus guias do quão boa sua busca está se saindo!\n",
    "\n",
    "Como métrica de avaliação, esta competição irá utilizar a média do NDCG calculado para cada query. \n",
    "\n",
    "## Ferramental\n",
    "Para executar este lab não é necessário uma máquina com GPU, mas a máquina deve ter capacidade de virtualização, além de possuir o Docker e o Python instalado.\n",
    "\n",
    "Será utilizado o ElasticSearch (e opcionalmente o Kibana) no ambiente docker.\n",
    "\n",
    "Para realizar o lab é recomendado o uso de Python 3.12, além de um venv ou ambiente conda.\n",
    "\n",
    "Você deve instalar as dependências do requirements.txt.\n",
    "\n",
    "## Detalhes\n",
    "Ao executar o docker compose, além de ser levantado o ElasticSearch, também é levantada uma interface visual chamada Kibana.\n",
    "\n",
    "O Kibana é um frontend para facilitar o acionamento de algumas operações do ElasticSearch e controle de configurações específicas, observar métricas etc. Ela pode ser utilizada para fins de debug. Para quem tiver curiosidade, acesse o URI: http://localhost:5601 após levantar o serviço com o docker compose.\n",
    "\n",
    "## Dúvidas\n",
    "Caso tenham dúvidas, é só entrar em contato pelo nosso servidor do Discord."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Passos de preparação do ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(bash) Para instalar os requirements em um venv, execute os comandos abaixo a partir do diretório desse repositório\n",
    "\n",
    "```\n",
    "python3.12 -m venv elastic-lab\n",
    "source elastic-lab/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Para desativar o venv quando tiver terminado\n",
    "```\n",
    "deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/mignoe/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mignoe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import OrderedDict\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 0: Subir Stack do Elastic (ElasticSearch e Kibana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm # Caso o comando não funcione, execute-o no terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=\"2026-02-15T17:53:33-03:00\" level=warning msg=\"/home/mignoe/Documents/git/RecInfo_LabElasticSearch/docker-compose.yaml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion\"\n",
      "unable to get image 'docker.elastic.co/kibana/kibana:8.17.3': permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \"http://%2Fvar%2Frun%2Fdocker.sock/v1.51/images/docker.elastic.co/kibana/kibana:8.17.3/json\": dial unix /var/run/docker.sock: connect: permission denied\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call([\"docker\", \"compose\", \"up\", \"-d\"])\n",
    "# Se esse comando falhar ou retornar 1, execute-o diretamente no terminal para identificar o erro.\n",
    "# PS: O docker deve estar instalado e rodando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: Baixar dados do Lupa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de dados de notícias da Lupa\n",
    "url = \"https://docs.google.com/uc?export=download&confirm=t&id=1W067Md2EbvVzW1ufzFg17Hf7Y9cCZxxr\"\n",
    "filename = \"articles_lupa_lab_elasticsearch.zip\"\n",
    "data_path = \"data\"\n",
    "zip_file_path = f\"{data_path}/{filename}\"\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Baixa o zip\n",
    "with open(zip_file_path, \"wb\") as f:\n",
    "    f.write(requests.get(url, allow_redirects=True).content)\n",
    "\n",
    "# Extrai o csv do zip\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_path)\n",
    "    \n",
    "output_file = f\"{data_path}/articles_lupa.csv\"\n",
    "assert os.path.exists(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Pré-processar os dados e gerar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementações de pré-processamentos de texto. Modifiquem, adicionem, removam conforme necessário.\n",
    "class Preprocessors:\n",
    "    STOPWORDS = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.spacy_nlp = spacy.load(\"pt_core_news_sm\") # Utiliza para lematização\n",
    "        \n",
    "    # Remove stopwords do português\n",
    "    def remove_stopwords(self, text):\n",
    "        # Tokeniza as palavras\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove as stop words\n",
    "        tokens = [word for word in tokens if word not in self.STOPWORDS]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    # Realiza a lematização\n",
    "    def lemma(self, text):\n",
    "        return \" \".join([token.lemma_ for token in self.spacy_nlp(text)])\n",
    "    \n",
    "    # Realiza a stemização\n",
    "    def porter_stemmer(self, text):\n",
    "        # Tokeniza as palavras\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        for index in range(len(tokens)):\n",
    "            # Realiza a stemização\n",
    "            stem_word = self.stemmer.stem(tokens[index])\n",
    "            tokens[index] = stem_word\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Transforma o texto em lower case\n",
    "    def lower_case(self, str):\n",
    "        return str.lower()\n",
    "\n",
    "    # Remove urls com regex\n",
    "    def remove_urls(self, text):\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
    "        return without_urls\n",
    "\n",
    "    # Remove números com regex\n",
    "    def remove_numbers(self, text):\n",
    "        number_pattern = r'\\d+'\n",
    "        without_number = re.sub(pattern=number_pattern,\n",
    "    repl=\" \", string=text)\n",
    "        return without_number\n",
    "\n",
    "    # Converte caracteres acentuados para sua versão ASCII\n",
    "    def accented_to_ascii(self, text):\n",
    "        text = unidecode.unidecode(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geração de embeddings finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo gerador de embeddings\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Caminho para salvar o dataframe de notícias\n",
    "data_df_path = \"data/data_df.pkl\"\n",
    "\n",
    "# Selecione diferentes pré-processamentos\n",
    "# Exemplo:\n",
    "\"\"\"\n",
    "preprocessor = Preprocessors()\n",
    "preprocessing_steps = [\n",
    "    preprocessor.remove_urls,\n",
    "    preprocessor.remove_stopwords,\n",
    "]\n",
    "\"\"\"\n",
    "preprocessor = Preprocessors()\n",
    "preprocessing_steps = [\n",
    "    # Adicione os pré-processamentos aqui\n",
    "    preprocessor.remove_stopwords,\n",
    "    preprocessor.lemma,\n",
    "    preprocessor.lower_case\n",
    "    \n",
    "]\n",
    "\n",
    "RECREATE_DF = True\n",
    "\n",
    "# Cria o data frame se ele já existir ou se a variável RECREATE_INDEX for verdadeira\n",
    "# Ou (exclusivo) carrega o dataframe salvo\n",
    "if not os.path.exists(data_df_path) or RECREATE_DF:    \n",
    "    df = pd.read_csv(output_file, sep=\";\")[[\"Título\", \"Texto\", \"Data de Publicação\"]]\n",
    "    df[\"Data de Publicação\"] = df[\"Data de Publicação\"].apply(lambda str_date: datetime.strptime(str_date.split(\" - \")[0], \"%d.%m.%Y\"))\n",
    "    df.sort_values(\"Data de Publicação\", inplace=True, ascending=False)\n",
    "\n",
    "    df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    df[\"Embeddings\"] = [None] * len(df)\n",
    "    df[\"doc_id\"] = df.reset_index(drop=True).index\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        texto_completo = row[\"Texto\"].strip() + \"\\n\" + row[\"Título\"].strip()\n",
    "        \n",
    "        df.at[i, \"Texto completo\"] = texto_completo\n",
    "        texto_processado = texto_completo\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            texto_processado = preprocessing_step(texto_processado)\n",
    "        \n",
    "        df.at[i, \"Texto processado\"] = texto_processado\n",
    "        embeddings = model.encode(texto_completo).tolist()\n",
    "        df.at[i, \"Embeddings\"] = embeddings\n",
    "        \n",
    "    print(\"Geração de embeddings finalizada.\")\n",
    "    \n",
    "    with open(data_df_path, \"wb\") as f:\n",
    "        df.to_pickle(f)\n",
    "else:\n",
    "    with open(data_df_path, \"rb\") as f:\n",
    "        df = pd.read_pickle(f)\n",
    "    print(\"Dataframe carregado de arquivo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 3: Indexar dados no ElasticSearch (Lembrem-se de reindexar os dados se os pré-processamentos mudarem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts = [{'host': \"localhost\", 'port': 9200, \"scheme\": \"https\"}],\n",
    "    basic_auth=(\"elastic\",\"elastic\"),\n",
    "    verify_certs = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice 'verificacoes_lupa' deletado.\n",
      "Índice 'verificacoes_lupa' criado.\n",
      "Índice preenchido.\n",
      "Indexação finalizada.\n"
     ]
    }
   ],
   "source": [
    "RECREATE_INDEX = True\n",
    "\n",
    "index_name = \"verificacoes_lupa\"\n",
    "\n",
    "# Se a flag for True e se o índice existir, ele é deletado\n",
    "if RECREATE_INDEX and es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Índice '{index_name}' deletado.\")\n",
    "\n",
    "# Cria o índice e popula com os dados\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, mappings={\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"integer\"},\n",
    "            \"full_text\": {\"type\": \"text\"},\n",
    "            \"processed_text\": {\"type\": \"text\"},\n",
    "            \"embeddings\": {\"type\": \"dense_vector\", \"dims\": 384}\n",
    "        }\n",
    "    })\n",
    "    print(f\"Índice '{index_name}' criado.\")\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        es.index(index=index_name, id=row[\"doc_id\"], body={\n",
    "            \"doc_id\": row[\"doc_id\"],\n",
    "            \"full_text\": row[\"Texto completo\"],\n",
    "            \"processed_text\": row[\"Texto processado\"],\n",
    "            \"embeddings\": row[\"Embeddings\"]\n",
    "        })\n",
    "    print(\"Índice preenchido.\")\n",
    "\n",
    "print(\"Indexação finalizada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 1: Criar query\n",
    "Crie as queries da tarefa 1 (QP1 e QP2), as queries devem ser perguntas ou declarações de até 100 caracteres.\n",
    "\n",
    "Inspire-se nas notícias presente no csv (data/articles_lupa.csv) para gerar queries interessantes.\n",
    "\n",
    "Submetam as queries ao formulário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 2: Implementação e realização das buscas\n",
    "Agora que você montou suas queries, realize cada uma das buscas para cada uma das queries (QF1; QF2; QP1 e QP2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas serão as queries QF1 e QF2\n",
    "with open(\"data/queries_fixadas.txt\", \"r\") as f:\n",
    "    queries_fixadas = [line.strip() for line in f.readlines()]\n",
    "    assert len(queries_fixadas) == 2\n",
    "    QF1 = queries_fixadas[0]\n",
    "    QF2 = queries_fixadas[1]\n",
    "    \n",
    "# Preencha aqui as queries do grupo\n",
    "QP1 = \"Avanço da ultra direita na Argentina são um alerta para o avanço do radicalismo político.\"\n",
    "QP2 = \"Receitas caseiras combatem a dengue.\"\n",
    "\n",
    "queries = OrderedDict()\n",
    "queries[\"QF1\"] = QF1\n",
    "queries[\"QF2\"] = QF2\n",
    "queries[\"QP1\"] = QP1\n",
    "queries[\"QP2\"] = QP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Léxica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação de busca esparsa (léxica) com BM25\n",
    "def lexical_search(queries: dict[str, str]):\n",
    "    lexical_results = {}\n",
    "    for query_id, query in queries.items():\n",
    "        \n",
    "        # Pré-processa os dados\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            query = preprocessing_step(query)\n",
    "        \n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"processed_text\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Realiza a busca\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        \n",
    "        hits_results = []\n",
    "        # Recupera os resultados\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            hits_results.append((hit[\"_source\"][\"doc_id\"], hit[\"_score\"]))\n",
    "        lexical_results[query_id] = hits_results\n",
    "        \n",
    "    return lexical_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza busca semântica (densa) com KNN exato\n",
    "def semantic_search(queries: dict[str, str]):\n",
    "    semantic_results = {}\n",
    "    \n",
    "    for query_id, query_text in queries.items():\n",
    "        # Aplica todos os pré-processamentos aos dados\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            query_text = preprocessing_step(query_text)\n",
    "            \n",
    "        query_vector = model.encode(query_text).tolist()\n",
    "        \n",
    "        \n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, 'embeddings') + 1.0\",\n",
    "                        \"params\": {\"query_vector\": query_vector}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Realiza a busca\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        \n",
    "        hits_results = []\n",
    "        # Recupera top 10 resultados\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            hits_results.append((hit[\"_source\"][\"doc_id\"], hit[\"_score\"]))\n",
    "            \n",
    "        semantic_results[query_id] = hits_results\n",
    "\n",
    "    return semantic_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Híbrida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca híbrida ou RRF. Implemente sua solução aqui. Você pode realizar as duas buscas anteriores (léxica e semântica) como base para formar a busca híbrida.\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def compute_RRF(hits_search, k=60): \n",
    "    return {doc_id: (1 / (k + position_rank)) for position_rank, (doc_id, _) in enumerate(hits_search)}\n",
    "\n",
    "\n",
    "def hybrid_search(queries: dict[str, str]):\n",
    "\n",
    "    semantic_search_results, lexical_search_results = semantic_search(queries), lexical_search(queries)\n",
    "\n",
    "    hybrid_results = {}\n",
    "\n",
    "    for (query_id, _) in queries.items():\n",
    "\n",
    "        lexical_hits = lexical_search_results.get(query_id)\n",
    "        semantic_hits = semantic_search_results.get(query_id)\n",
    "\n",
    "        documents_score = dict()\n",
    "        for doc_id, score in compute_RRF(lexical_hits).items():\n",
    "            documents_score[doc_id] = score\n",
    "        \n",
    "        for doc_id, score in compute_RRF(semantic_hits).items():\n",
    "            if doc_id in documents_score.keys():\n",
    "                documents_score[doc_id] += score\n",
    "            else:\n",
    "                documents_score[doc_id] = score\n",
    "\n",
    "\n",
    "        hybrid_results[query_id] = sorted(documents_score.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    return hybrid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Criativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente sua própria estratégia de busca, podendo ela ser esparsa, densa ou híbrida. Implemente algo como \"more_like_this\", \"BM35\", \"fuzzy\" etc.\n",
    "def creative_search(queries: dict[str, str]):\n",
    "    ## TODO: Implementar busca híbrida\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução das buscas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_functions = [\n",
    "    (\"lexical\", lexical_search),\n",
    "    (\"semantic\", semantic_search),\n",
    "    # (\"hybrid\", hybrid_search),\n",
    "    # (\"creative\", creative_search)\n",
    "]\n",
    "\n",
    "def run_all_searches(queries: dict[str, str]):\n",
    "    all_search_results = {}\n",
    "    for search_name, search_function in search_functions:\n",
    "        results = search_function(queries)\n",
    "        all_search_results[search_name] = results\n",
    "    return all_search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analise os resultados da busca e aprimore a busca!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>[(1429, 16.931242), (299, 16.6744), (140, 16.4...</td>\n",
       "      <td>[(1398, 1.6568059), (2188, 1.5682691), (1429, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>[(242, 11.065164), (440, 10.894832), (1621, 10...</td>\n",
       "      <td>[(1237, 1.8280691), (1722, 1.7961174), (510, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>[(1022, 29.90496), (1676, 16.87691), (692, 15....</td>\n",
       "      <td>[(140, 1.6776567), (470, 1.6299429), (1922, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>[(899, 13.261494), (1363, 12.721932), (48, 12....</td>\n",
       "      <td>[(1059, 1.8277775), (277, 1.7721117), (1363, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lexical  \\\n",
       "QF1  [(1429, 16.931242), (299, 16.6744), (140, 16.4...   \n",
       "QF2  [(242, 11.065164), (440, 10.894832), (1621, 10...   \n",
       "QP1  [(1022, 29.90496), (1676, 16.87691), (692, 15....   \n",
       "QP2  [(899, 13.261494), (1363, 12.721932), (48, 12....   \n",
       "\n",
       "                                              semantic  \n",
       "QF1  [(1398, 1.6568059), (2188, 1.5682691), (1429, ...  \n",
       "QF2  [(1237, 1.8280691), (1722, 1.7961174), (510, 1...  \n",
       "QP1  [(140, 1.6776567), (470, 1.6299429), (1922, 1....  \n",
       "QP2  [(1059, 1.8277775), (277, 1.7721117), (1363, 1...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_results = run_all_searches(queries)\n",
    "search_results_df = pd.DataFrame(all_search_results)\n",
    "search_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados das buscas salvos em 'data/search_results.csv'.\n",
      "Documentos de interesse salvos em 'data/documents.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1429</td>\n",
       "      <td>1398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>299</td>\n",
       "      <td>2188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>140</td>\n",
       "      <td>1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1689</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>2328</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1398</td>\n",
       "      <td>1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1735</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>410</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1887</td>\n",
       "      <td>1182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>2021</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>242</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>440</td>\n",
       "      <td>1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1621</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1594</td>\n",
       "      <td>1132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1237</td>\n",
       "      <td>1621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>316</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>15</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>2208</td>\n",
       "      <td>1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>510</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1067</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>1022</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>1676</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>692</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>1013</td>\n",
       "      <td>1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>2357</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>1833</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>1309</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>1807</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>928</td>\n",
       "      <td>1725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>438</td>\n",
       "      <td>1327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>899</td>\n",
       "      <td>1059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1363</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>48</td>\n",
       "      <td>1363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1059</td>\n",
       "      <td>2343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>490</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>277</td>\n",
       "      <td>899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1185</td>\n",
       "      <td>1631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1067</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2343</td>\n",
       "      <td>2235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1719</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lexical  semantic\n",
       "QF1     1429      1398\n",
       "QF1      299      2188\n",
       "QF1      140      1429\n",
       "QF1     1689      1887\n",
       "QF1     2328       487\n",
       "QF1     1398      1125\n",
       "QF1     1735      1300\n",
       "QF1      410       333\n",
       "QF1     1887      1182\n",
       "QF1     2021       177\n",
       "QF2      242      1237\n",
       "QF2      440      1722\n",
       "QF2     1621       510\n",
       "QF2     1594      1132\n",
       "QF2     1237      1621\n",
       "QF2      316      1989\n",
       "QF2       15       861\n",
       "QF2     2208      1027\n",
       "QF2      510      1956\n",
       "QF2     1067       323\n",
       "QP1     1022       140\n",
       "QP1     1676       470\n",
       "QP1      692      1922\n",
       "QP1     1013      1107\n",
       "QP1     2357       686\n",
       "QP1     1833        86\n",
       "QP1     1309      1022\n",
       "QP1     1807      1436\n",
       "QP1      928      1725\n",
       "QP1      438      1327\n",
       "QP2      899      1059\n",
       "QP2     1363       277\n",
       "QP2       48      1363\n",
       "QP2     1059      2343\n",
       "QP2      490      1067\n",
       "QP2      277       899\n",
       "QP2     1185      1631\n",
       "QP2     1067       279\n",
       "QP2     2343      2235\n",
       "QP2     1719      2525"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_exploded_df(search_results_df):\n",
    "    exploded_search_results_df = pd.concat([search_results_df[col].explode() for col in search_results_df.columns], axis=1)\n",
    "    exploded_search_results_df = exploded_search_results_df.apply(lambda l: [doc_id for doc_id, _ in l])\n",
    "    return exploded_search_results_df\n",
    "\n",
    "def generate_found_docs_text_df(exploded_search_results_df, all_docs_df):\n",
    "    # Recupera os ids únicos dos documentos\n",
    "    documents_ids = set(exploded_search_results_df.to_numpy().flatten().tolist())\n",
    "\n",
    "    # Salva os textos e os ids dos documetnos que foram encontrados ems usas buscas\n",
    "    documents_df = all_docs_df[all_docs_df[\"doc_id\"].isin(documents_ids)][[\"Texto processado\", \"doc_id\"]]\n",
    "    return documents_df\n",
    "\n",
    "exploded_df = generate_exploded_df(search_results_df)\n",
    "found_docs_text_df = generate_found_docs_text_df(exploded_df, all_docs_df=df)\n",
    "\n",
    "def save_results_to_file(exploded_df: pd.DataFrame,\n",
    "                         found_docs_text_df: pd.DataFrame,\n",
    "                         exploded_df_save_filepath: str = \"data/search_results.csv\",\n",
    "                         found_docs_text_save_filepath: str = \"data/documents.csv\"):\n",
    "    exploded_df.to_csv(exploded_df_save_filepath)\n",
    "    found_docs_text_df.to_csv(found_docs_text_save_filepath)\n",
    "    print(\"Resultados das buscas salvos em 'data/search_results.csv'.\")\n",
    "    print(\"Documentos de interesse salvos em 'data/documents.csv'.\")\n",
    "    \n",
    "save_results_to_file(exploded_df, found_docs_text_df)\n",
    "exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>É falso que houve megaprotesto na Alemanha con...</td>\n",
       "      <td>circular rede social foto mostrar dezena carro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Estudo não aponta que carga viral de vacinados...</td>\n",
       "      <td>circular rede social estudo publicar revista c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>É antiga foto de manifestante em cima de carro...</td>\n",
       "      <td>circular rede social foto homem mascarar , seg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Diretor da Anvisa não pediu demissão e critico...</td>\n",
       "      <td>circular rede social diretor anvisa , agência ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Jogadoras da seleção de futebol não posaram in...</td>\n",
       "      <td>circular rede imagem dois jogadora seleção bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>2565</td>\n",
       "      <td>Es falso que farmacias de Italia estén distrib...</td>\n",
       "      <td>circula en las rede sociales las farmacias en ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>2566</td>\n",
       "      <td>Foto viral que mostra Faixa de Gaza após bomba...</td>\n",
       "      <td>circular rede social foto mostrar cidade céu c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>2567</td>\n",
       "      <td>É falso que STF afastou Bolsonaro do controle ...</td>\n",
       "      <td>circular rede social supremo tribunal federal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>2568</td>\n",
       "      <td>É falso que Magazine Luiza 'financia' fome dos...</td>\n",
       "      <td>em o comissão parlamentar inquérito ( cpi ) on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>2569</td>\n",
       "      <td>Como a nova faixa de isenção impactará no Impo...</td>\n",
       "      <td>em 18 fevereiro , receita federal anunciar mud...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2570 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc_id                                              title  \\\n",
       "0          0  É falso que houve megaprotesto na Alemanha con...   \n",
       "1          1  Estudo não aponta que carga viral de vacinados...   \n",
       "2          2  É antiga foto de manifestante em cima de carro...   \n",
       "3          3  Diretor da Anvisa não pediu demissão e critico...   \n",
       "4          4  Jogadoras da seleção de futebol não posaram in...   \n",
       "...      ...                                                ...   \n",
       "2565    2565  Es falso que farmacias de Italia estén distrib...   \n",
       "2566    2566  Foto viral que mostra Faixa de Gaza após bomba...   \n",
       "2567    2567  É falso que STF afastou Bolsonaro do controle ...   \n",
       "2568    2568  É falso que Magazine Luiza 'financia' fome dos...   \n",
       "2569    2569  Como a nova faixa de isenção impactará no Impo...   \n",
       "\n",
       "                                                content  \n",
       "0     circular rede social foto mostrar dezena carro...  \n",
       "1     circular rede social estudo publicar revista c...  \n",
       "2     circular rede social foto homem mascarar , seg...  \n",
       "3     circular rede social diretor anvisa , agência ...  \n",
       "4     circular rede imagem dois jogadora seleção bra...  \n",
       "...                                                 ...  \n",
       "2565  circula en las rede sociales las farmacias en ...  \n",
       "2566  circular rede social foto mostrar cidade céu c...  \n",
       "2567  circular rede social supremo tribunal federal ...  \n",
       "2568  em o comissão parlamentar inquérito ( cpi ) on...  \n",
       "2569  em 18 fevereiro , receita federal anunciar mud...  \n",
       "\n",
       "[2570 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_id_map(all_docs_df, output_csv_path):\n",
    "    \"\"\"\n",
    "    Exports a stable mapping of ALL documents used in Elasticsearch.\n",
    "\n",
    "    The doc_id values will exactly match those returned by search results.\n",
    "    \"\"\"\n",
    "\n",
    "    required_cols = {\"doc_id\", \"Título\", \"Texto processado\"}\n",
    "    missing = required_cols - set(all_docs_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    id_map_df = (\n",
    "        all_docs_df[[\"doc_id\", \"Título\", \"Texto processado\"]]\n",
    "        .rename(columns={\n",
    "            \"Título\": \"title\",\n",
    "            \"Texto processado\": \"content\"\n",
    "        })\n",
    "        .sort_values(\"doc_id\")   # optional, but good for reproducibility\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    id_map_df.to_csv(output_csv_path, sep=\";\", index=False)\n",
    "\n",
    "    return id_map_df\n",
    "\n",
    "generate_id_map(df, \"data/id_map.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anote as relevâncias na sua planilha!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 3: Reexecutar a busca para as novas queries e rotular os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>[(1429, 16.78515), (299, 16.539219), (140, 16....</td>\n",
       "      <td>[(1398, 1.6568059), (2188, 1.5682691), (1429, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>[(242, 11.126562), (440, 10.944557), (1621, 10...</td>\n",
       "      <td>[(1237, 1.8280691), (1722, 1.7961174), (510, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>[(1781, 11.784787), (913, 11.599973), (1797, 8...</td>\n",
       "      <td>[(2225, 1.506655), (1092, 1.5009346), (1239, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>[(1489, 27.033447), (2063, 9.170925), (143, 5....</td>\n",
       "      <td>[(2452, 1.3766779), (2159, 1.3721855), (168, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>[(2392, 7.967837), (2176, 7.784437), (1108, 7....</td>\n",
       "      <td>[(1014, 1.3425832), (2126, 1.3414716), (1464, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lexical  \\\n",
       "QF1  [(1429, 16.78515), (299, 16.539219), (140, 16....   \n",
       "QF2  [(242, 11.126562), (440, 10.944557), (1621, 10...   \n",
       "QA1  [(1781, 11.784787), (913, 11.599973), (1797, 8...   \n",
       "QA2  [(1489, 27.033447), (2063, 9.170925), (143, 5....   \n",
       "QA3  [(2392, 7.967837), (2176, 7.784437), (1108, 7....   \n",
       "\n",
       "                                              semantic  \n",
       "QF1  [(1398, 1.6568059), (2188, 1.5682691), (1429, ...  \n",
       "QF2  [(1237, 1.8280691), (1722, 1.7961174), (510, 1...  \n",
       "QA1  [(2225, 1.506655), (1092, 1.5009346), (1239, 1...  \n",
       "QA2  [(2452, 1.3766779), (2159, 1.3721855), (168, 1...  \n",
       "QA3  [(1014, 1.3425832), (2126, 1.3414716), (1464, ...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preencha aqui com as queries adicionais do seu grupo\n",
    "QA1 = \"O lula se embriaga com frequência?\"\n",
    "QA2 = 'O Bolsonaro entrou no covil dos ursos?'\n",
    "QA3 = \"T?\"\n",
    "\n",
    "queries = OrderedDict()\n",
    "queries[\"QF1\"] = QF1\n",
    "queries[\"QF2\"] = QF2\n",
    "queries[\"QA1\"] = QA1\n",
    "queries[\"QA2\"] = QA2\n",
    "queries[\"QA3\"] = QA3\n",
    "\n",
    "\n",
    "all_search_results = run_all_searches(queries)\n",
    "search_results_df = pd.DataFrame(all_search_results)\n",
    "search_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados das buscas salvos em 'data/search_results.csv'.\n",
      "Documentos de interesse salvos em 'data/documents.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1429</td>\n",
       "      <td>1398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>299</td>\n",
       "      <td>2188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>140</td>\n",
       "      <td>1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1689</td>\n",
       "      <td>2555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>2328</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1398</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1735</td>\n",
       "      <td>1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>410</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1887</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>2021</td>\n",
       "      <td>1182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>242</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>440</td>\n",
       "      <td>1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1621</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1594</td>\n",
       "      <td>1132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1237</td>\n",
       "      <td>1621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>316</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>15</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>2208</td>\n",
       "      <td>1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>510</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1067</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>1781</td>\n",
       "      <td>2225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>913</td>\n",
       "      <td>1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>1797</td>\n",
       "      <td>1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>1409</td>\n",
       "      <td>804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>388</td>\n",
       "      <td>1417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>904</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>617</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>339</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>1107</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA1</th>\n",
       "      <td>2354</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>1489</td>\n",
       "      <td>2452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>2063</td>\n",
       "      <td>2159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>143</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>129</td>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>1410</td>\n",
       "      <td>2111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>1162</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>201</td>\n",
       "      <td>1538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>31</td>\n",
       "      <td>2219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>239</td>\n",
       "      <td>1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA2</th>\n",
       "      <td>107</td>\n",
       "      <td>1379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>2392</td>\n",
       "      <td>1014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>2176</td>\n",
       "      <td>2126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>1108</td>\n",
       "      <td>1464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>2519</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>1789</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>2149</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>2078</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>2111</td>\n",
       "      <td>2408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>43</td>\n",
       "      <td>1172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA3</th>\n",
       "      <td>2441</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lexical  semantic\n",
       "QF1     1429      1398\n",
       "QF1      299      2188\n",
       "QF1      140      1429\n",
       "QF1     1689      2555\n",
       "QF1     2328      1887\n",
       "QF1     1398       487\n",
       "QF1     1735      1125\n",
       "QF1      410      1300\n",
       "QF1     1887       333\n",
       "QF1     2021      1182\n",
       "QF2      242      1237\n",
       "QF2      440      1722\n",
       "QF2     1621       510\n",
       "QF2     1594      1132\n",
       "QF2     1237      1621\n",
       "QF2      316      1989\n",
       "QF2       15       861\n",
       "QF2     2208      1027\n",
       "QF2      510      1956\n",
       "QF2     1067       323\n",
       "QA1     1781      2225\n",
       "QA1      913      1092\n",
       "QA1     1797      1239\n",
       "QA1     1409       804\n",
       "QA1      388      1417\n",
       "QA1      904        78\n",
       "QA1      617       396\n",
       "QA1      339      1359\n",
       "QA1     1107       980\n",
       "QA1     2354        48\n",
       "QA2     1489      2452\n",
       "QA2     2063      2159\n",
       "QA2      143       168\n",
       "QA2      129      1063\n",
       "QA2     1410      2111\n",
       "QA2     1162       197\n",
       "QA2      201      1538\n",
       "QA2       31      2219\n",
       "QA2      239      1720\n",
       "QA2      107      1379\n",
       "QA3     2392      1014\n",
       "QA3     2176      2126\n",
       "QA3     1108      1464\n",
       "QA3     2519       824\n",
       "QA3     1789      1078\n",
       "QA3     2149       714\n",
       "QA3     2078       929\n",
       "QA3     2111      2408\n",
       "QA3       43      1172\n",
       "QA3     2441       709"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_df = generate_exploded_df(search_results_df)\n",
    "found_docs_text_df = generate_found_docs_text_df(exploded_df, all_docs_df=df)\n",
    "\n",
    "def save_results_to_file(exploded_df: pd.DataFrame,\n",
    "                         found_docs_text_df: pd.DataFrame,\n",
    "                         exploded_df_save_filepath: str = \"data/search_results.csv\",\n",
    "                         found_docs_text_save_filepath: str = \"data/documents.csv\"):\n",
    "    exploded_df.to_csv(exploded_df_save_filepath)\n",
    "    found_docs_text_df.to_csv(found_docs_text_save_filepath)\n",
    "    print(\"Resultados das buscas salvos em 'data/search_results.csv'.\")\n",
    "    print(\"Documentos de interesse salvos em 'data/documents.csv'.\")\n",
    "    \n",
    "save_results_to_file(exploded_df, found_docs_text_df)\n",
    "exploded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anote os resultados na sua planilha!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A competição utilizará o NDCG médio por query para computar seu desempenho."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
